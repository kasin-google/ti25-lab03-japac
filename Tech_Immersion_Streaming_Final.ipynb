{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#4285f4'>Prerequisite to run this notebook</font>"
      ],
      "metadata": {
        "id": "NFGz8rDcnk7v"
      },
      "id": "NFGz8rDcnk7v"
    },
    {
      "cell_type": "code",
      "source": [
        "#enable kafka API in the project\n",
        "!(gcloud services enable managedkafka.googleapis.com --project \"${GOOGLE_CLOUD_PROJECT}\")"
      ],
      "metadata": {
        "id": "2pTrO3XSnzNf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2pTrO3XSnzNf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99cafad5-e5c3-4136-a05f-31ae7aea55fc",
      "metadata": {
        "tags": [],
        "id": "99cafad5-e5c3-4136-a05f-31ae7aea55fc"
      },
      "outputs": [],
      "source": [
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "kPYZ_UJ_cAR2"
      },
      "id": "kPYZ_UJ_cAR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation"
      ],
      "metadata": {
        "id": "_dv8JuvQnd-k"
      },
      "id": "_dv8JuvQnd-k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9141ccc8-0bcd-4b63-b0b1-d8c21552dc43",
      "metadata": {
        "id": "9141ccc8-0bcd-4b63-b0b1-d8c21552dc43"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdf3820-4a80-4526-81a9-6c72d48d11a1",
      "metadata": {
        "tags": [],
        "id": "ccdf3820-4a80-4526-81a9-6c72d48d11a1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = \"us-central1\"\n",
        "BQ_LOCATION = \"us\"\n",
        "DATASET_ID = \"cymbal_consumer_finance\"\n",
        "DATA_BUCKET_NAME = PROJECT_ID\n",
        "\n",
        "kafka_cluster_name = \"ti-kafka-cluster-01\"\n",
        "kafka_topic_name = \"customer-events-topic-01\"\n",
        "\n",
        "#change <YOUR-LDAP> to your real ldap or pick any other name\n",
        "dataflow_bucket = \"ti-dataflow-staging-<YOUR-LDAP>\"# should not have logical delete on\n",
        "dataflow_service_account = \"ti-dataflow-service-kafka-<YOUR-LDAP>@<YOUR-LDAP>.iam.gserviceaccount.com\" # Needs Role: roles/managedkafka.client\n",
        "#the account running this notebook needs to have \"Service Account User\" on this account.\n",
        "\n",
        "CONNECTION_NAME = \"vertex-ai\"\n",
        "connection = f\"{PROJECT_ID}.{REGION}.{CONNECTION_NAME}\"\n",
        "\n",
        "#change this\n",
        "network=\"default\"\n",
        "#network=\"colab-network\"\n",
        "subnet = \"default\"\n",
        "#subnet = \"colab-subnetwork\"\n",
        "\n",
        "#TODO: change this hardcoding.\n",
        "life_events = [\"New Child\",\n",
        "               \"Graduation\",\n",
        "               \"Relocation\",\n",
        "               \"Retirement\",\n",
        "               \"Home Purchase\",\n",
        "               \"Medical Event\",\n",
        "               \"Starting Business\",\n",
        "               \"Marriage\",\n",
        "               \"Divorce\" ]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(gcloud storage buckets create gs://{dataflow_bucket} \\\n",
        "    --project=\"{PROJECT_ID}\")"
      ],
      "metadata": {
        "id": "1kTEi_F4nQDT"
      },
      "id": "1kTEi_F4nQDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(gcloud storage buckets create gs://{DATA_BUCKET_NAME} \\\n",
        "    --project=\"{PROJECT_ID}\")"
      ],
      "metadata": {
        "id": "Qawdlz52UEap"
      },
      "id": "Qawdlz52UEap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(gcloud compute networks subnets create \"{subnet}\" \\\n",
        "    --project=\"{PROJECT_ID}\" \\\n",
        "    --region=\"{REGION}\" \\\n",
        "    --network=\"{network}\" \\\n",
        "    --range=\"10.10.0.0/20\")"
      ],
      "metadata": {
        "id": "1zt35KD0cp99"
      },
      "id": "1zt35KD0cp99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d43d36-5f38-4b96-ab47-d731e5fae390",
      "metadata": {
        "tags": [],
        "id": "c4d43d36-5f38-4b96-ab47-d731e5fae390"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "client = bigquery.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65fff23c-4782-4324-93e6-0684d8476d11",
      "metadata": {
        "id": "65fff23c-4782-4324-93e6-0684d8476d11"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf65973d-0c28-4cfe-b7ad-b77c286f8d8d",
      "metadata": {
        "tags": [],
        "id": "cf65973d-0c28-4cfe-b7ad-b77c286f8d8d"
      },
      "outputs": [],
      "source": [
        "def PrettyPrintJson(json_string):\n",
        "  json_object = json.loads(json_string)\n",
        "  json_formatted_str = json.dumps(json_object, indent=2)\n",
        "  print(json_formatted_str)\n",
        "  return json.dumps(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e8af8f-9232-468f-b714-fb51a1a37936",
      "metadata": {
        "tags": [],
        "id": "28e8af8f-9232-468f-b714-fb51a1a37936"
      },
      "outputs": [],
      "source": [
        "def restAPIHelper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return json.loads(response.content)\n",
        "    #image_data = json.loads(response.content)[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "  else:\n",
        "    error = f\"Error restAPIHelper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdvFzxQrhEel"
      },
      "source": [
        "### IAM Functions"
      ],
      "id": "rdvFzxQrhEel"
    },
    {
      "cell_type": "code",
      "source": [
        "def setProjectLevelIamPolicy(accountWithPrefix, role):\n",
        "  \"\"\"Sets the Project Level IAM policy.\"\"\"\n",
        "\n",
        "  # Get the current bindings (if the account has access then skip)\n",
        "  # https://cloud.google.com/resource-manager/reference/rest/v1/projects/getIamPolicy\n",
        "  project_id = f\"{PROJECT_ID}\"\n",
        "\n",
        "  url = f\"https://cloudresourcemanager.googleapis.com/v1/projects/{project_id}:getIamPolicy\"\n",
        "\n",
        "  request_body = { }\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "  print(f\"setProjectLevelIamPolicy (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if permissions exist\n",
        "  if \"bindings\" in json_result:\n",
        "    for item in json_result[\"bindings\"]:\n",
        "      if item[\"role\"] == role:\n",
        "        members = item[\"members\"]\n",
        "        for member in members:\n",
        "          if member == accountWithPrefix:\n",
        "            print(\"Permissions exist\")\n",
        "            return\n",
        "\n",
        "  # Take the existing bindings and we need to append the new permission\n",
        "  # Otherwise we loose the existing permissions\n",
        "  if \"bindings\" in json_result:\n",
        "    bindings = json_result[\"bindings\"]\n",
        "  else:\n",
        "    bindings = []\n",
        "\n",
        "  new_permission = {\n",
        "      \"role\": role,\n",
        "      \"members\": [ accountWithPrefix ]\n",
        "      }\n",
        "\n",
        "  bindings.append(new_permission)\n",
        "\n",
        "  # https://cloud.google.com/resource-manager/reference/rest/v1/projects/setIamPolicy\n",
        "  url = f\"https://cloudresourcemanager.googleapis.com/v1/projects/{project_id}:setIamPolicy\"\n",
        "\n",
        "  request_body = { \"policy\" : {\n",
        "      \"bindings\" : bindings\n",
        "      }\n",
        "  }\n",
        "\n",
        "  print(f\"Permission bindings: {bindings}\")\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "  print()\n",
        "  print(f\"json_result: {json_result}\")\n",
        "  print()\n",
        "  print(f\"Project Level IAM Permissions set for {accountWithPrefix} {role}\")"
      ],
      "metadata": {
        "id": "kc5bfyoZfDd2"
      },
      "id": "kc5bfyoZfDd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createVertexAIConnection(vertex_ai_connection_name):\n",
        "  \"\"\"Creates a Vertex AI connection.\"\"\"\n",
        "\n",
        "  # First find the connection\n",
        "  # https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/list\n",
        "  project_id = f\"{PROJECT_ID}\"\n",
        "  bigquery_location = f\"{BQ_LOCATION}\"\n",
        "  url = f\"https://bigqueryconnection.googleapis.com/v1/projects/{project_id}/locations/{bigquery_location}/connections\"\n",
        "\n",
        "  # Gather existing connections\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createVertexAIConnection (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if connection exists, if so return\n",
        "  if \"connections\" in json_result:\n",
        "    for item in json_result[\"connections\"]:\n",
        "      print(f\"BigLake Connection: {item['name']}\")\n",
        "      # \"projects/756740881369/locations/us/connections/vertex-ai-notebook-connection\"\n",
        "      # NOTE: We cannot test the complete name since it contains the project number and not id\n",
        "      if item[\"name\"].endswith(f\"/locations/{bigquery_location}/connections/{vertex_ai_connection_name}\"):\n",
        "        print(\"Connection already exists\")\n",
        "        serviceAccountId = item[\"cloudResource\"][\"serviceAccountId\"]\n",
        "        return serviceAccountId\n",
        "\n",
        "  # Create the connection\n",
        "  # https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/create\n",
        "  print(\"Creating Vertex AI Connection\")\n",
        "\n",
        "  url = f\"https://bigqueryconnection.googleapis.com/v1/projects/{project_id}/locations/{bigquery_location}/connections?connectionId={vertex_ai_connection_name}\"\n",
        "\n",
        "  request_body = {\n",
        "      \"friendlyName\": \"{vertex_ai_connection_name}\",\n",
        "      \"description\": \"Vertex AI Colab Notebooks Connection for BQ\",\n",
        "      \"cloudResource\": {}\n",
        "  }\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "\n",
        "  serviceAccountId = json_result[\"cloudResource\"][\"serviceAccountId\"]\n",
        "  print(\"Vertex AI Connection created: \", serviceAccountId)\n",
        "  return serviceAccountId"
      ],
      "metadata": {
        "id": "_PqgDtfNdSUY"
      },
      "id": "_PqgDtfNdSUY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYzQFcwnXMMe"
      },
      "source": [
        "### BQ Functions"
      ],
      "id": "IYzQFcwnXMMe"
    },
    {
      "cell_type": "code",
      "source": [
        "def setBigQueryDatasetPolicy(account, role):\n",
        "  \"\"\"Sets the BigQuery Dataset IAM policy.\"\"\"\n",
        "\n",
        "  # Get the current bindings (if the account has access then skip)\n",
        "  # https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/get\n",
        "\n",
        "  url = f\"https://bigquery.googleapis.com/bigquery/v2/projects/{PROJECT_ID}/datasets/{DATASET_ID}\"\n",
        "\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"setBigQueryDatasetPolicy (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if permissions exist\n",
        "  if \"access\" in json_result:\n",
        "    for item in json_result[\"access\"]:\n",
        "      if \"userByEmail\" in item:\n",
        "        if item[\"userByEmail\"] == account and item[\"role\"] == role:\n",
        "          print(\"Permissions exist\")\n",
        "          return\n",
        "\n",
        "\n",
        "  # Take the existing bindings and we need to append the new permission\n",
        "  # Otherwise we loose the existing permissions\n",
        "  if \"access\" in json_result:\n",
        "    access = json_result[\"access\"]\n",
        "  else:\n",
        "    access = []\n",
        "\n",
        "  new_permission = {\n",
        "      \"role\": role,\n",
        "      \"userByEmail\": account\n",
        "      }\n",
        "\n",
        "  access.append(new_permission)\n",
        "\n",
        "  # https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/patch\n",
        "  url = f\"https://bigquery.googleapis.com/bigquery/v2/projects/{PROJECT_ID}/datasets/{DATASET_ID}\"\n",
        "\n",
        "  request_body = {\n",
        "      \"access\" : access\n",
        "      }\n",
        "\n",
        "  print(f\"Permission bindings: {access}\")\n",
        "\n",
        "  json_result = restAPIHelper(url, \"PATCH\", request_body)\n",
        "  print()\n",
        "  print(f\"json_result: {json_result}\")\n",
        "  print()\n",
        "  print(f\"BigQuery Dataset IAM Permissions set for {account} {role}\")\n"
      ],
      "metadata": {
        "id": "ser5Vgio6RX8"
      },
      "id": "ser5Vgio6RX8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e78f30bf-146e-4417-8f32-3cd0e1d0ae2d",
      "metadata": {
        "tags": [],
        "id": "e78f30bf-146e-4417-8f32-3cd0e1d0ae2d"
      },
      "outputs": [],
      "source": [
        "def GetTableSchema(dataset_name, table_name):\n",
        "  import io\n",
        "\n",
        "  dataset_ref = client.dataset(dataset_name, project=PROJECT_ID)\n",
        "  table_ref = dataset_ref.table(table_name)\n",
        "  table = client.get_table(table_ref)\n",
        "\n",
        "  f = io.StringIO(\"\")\n",
        "  client.schema_to_json(table.schema, f)\n",
        "  return f.getvalue()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9ac524-e803-4617-bb5d-9a7124caf095",
      "metadata": {
        "tags": [],
        "id": "2c9ac524-e803-4617-bb5d-9a7124caf095"
      },
      "outputs": [],
      "source": [
        "def RunQuery(sql):\n",
        "  import time\n",
        "  from google.cloud import bigquery\n",
        "  client = bigquery.Client()\n",
        "\n",
        "  if (sql.startswith(\"SELECT\") or sql.startswith(\"WITH\")):\n",
        "      df_result = client.query(sql).to_dataframe()\n",
        "      return df_result\n",
        "  else:\n",
        "    job_config = bigquery.QueryJobConfig(priority=bigquery.QueryPriority.INTERACTIVE)\n",
        "    query_job = client.query(sql, job_config=job_config)\n",
        "\n",
        "    # Check on the progress by getting the job's updated state.\n",
        "    query_job = client.get_job(\n",
        "        query_job.job_id, location=query_job.location\n",
        "    )\n",
        "    print(\"Job {} is currently in state {} with error result of {}\".format(query_job.job_id, query_job.state, query_job.error_result))\n",
        "\n",
        "    while query_job.state != \"DONE\":\n",
        "      time.sleep(2)\n",
        "      query_job = client.get_job(\n",
        "          query_job.job_id, location=query_job.location\n",
        "          )\n",
        "      print(\"Job {} is currently in state {} with error result of {}\".format(query_job.job_id, query_job.state, query_job.error_result))\n",
        "\n",
        "    if query_job.error_result == None:\n",
        "      return True\n",
        "    else:\n",
        "      raise Exception(query_job.error_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kafka Functions"
      ],
      "metadata": {
        "id": "SaBw-nDvXkNx"
      },
      "id": "SaBw-nDvXkNx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4073e6-e775-4bed-a452-62cb2cbe5d79",
      "metadata": {
        "tags": [],
        "id": "3a4073e6-e775-4bed-a452-62cb2cbe5d79"
      },
      "outputs": [],
      "source": [
        "def createApacheKafkaForBigQueryCluster():\n",
        "  \"\"\"Creates a Apache Kafka For BigQuery Cluster.\"\"\"\n",
        "\n",
        "  # First find the cluster if it exists\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/list\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/clusters\"\n",
        "\n",
        "  # Gather existing clusters\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if cluster exists, if so return\n",
        "  if \"clusters\" in json_result:\n",
        "    for item in json_result[\"clusters\"]:\n",
        "      print(f\"Apache Kafka for BigQuery: {item['name']}\")\n",
        "      # \"projects/${project_id}/locations/us-central1/clusters/kafka-cluster\"\n",
        "      if item[\"name\"] == f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}\":\n",
        "        print(\"Apache Kafka for BigQuery already exists\")\n",
        "        return f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}\"\n",
        "\n",
        "  # Create Apache Kafka For BigQuery Cluster\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/create\n",
        "  print(\"Creating Apache Kafka For BigQuery Cluster\")\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/clusters?clusterId={kafka_cluster_name}\"\n",
        "\n",
        "  # Larger Apache Kafka Cluster\n",
        "  # vcpuCount: 32 -> You can probably use less CPUs since they are mainly ideal\n",
        "  # memoryBytes: 34359738368 -> RAM was at 50% when doing 11,000 customers\n",
        "\n",
        "  request_body = {\n",
        "      \"capacityConfig\": {\n",
        "        \"vcpuCount\": \"3\",\n",
        "        \"memoryBytes\": \"3221225472\"\n",
        "      },\n",
        "      \"gcpConfig\": {\n",
        "          \"accessConfig\": {\n",
        "              \"networkConfigs\": {\n",
        "                  \"subnet\": f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{subnet}\"\n",
        "                  }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "\n",
        "  name = json_result[\"name\"]\n",
        "  done = json_result[\"done\"]\n",
        "  print(\"Apache Kafka for BigQuery created: \", name)\n",
        "  return f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def waitForApacheKafkaForBigQueryCluster(operation):\n",
        "  \"\"\"\n",
        "  Waits for an Apache Kafka For BigQuery Cluster to be Created.\n",
        "\n",
        "  opertion:\n",
        "    projects/${project_id}/locations/us-central1/operations/operation-1723064212031-61f1e264889a9-9e3a863b-90613855\n",
        "  \"\"\"\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/{operation}\"\n",
        "  max_retries = 100\n",
        "  attempt = 0\n",
        "\n",
        "  while True:\n",
        "    # Gather existing connections\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "    print(f\"waitForApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "\n",
        "    # Test to see if connection exists, if so return\n",
        "    if \"state\" in json_result:\n",
        "      if json_result[\"state\"] == \"ACTIVE\":\n",
        "        print(\"Apache Kafka for BigQuery Cluster created\")\n",
        "        return None\n",
        "\n",
        "    # Wait for 10 seconds\n",
        "    attempt += 1\n",
        "    if attempt > max_retries:\n",
        "      raise RuntimeError(\"Apache Kafka for BigQuery Cluster not created\")\n",
        "    time.sleep(30)\n"
      ],
      "metadata": {
        "id": "DRi8OBFY_URC"
      },
      "id": "DRi8OBFY_URC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createApacheKafkaForBigQueryTopic():\n",
        "  \"\"\"Creates a Apache Kafka For BigQuery Topic.\"\"\"\n",
        "\n",
        "  # First find the topic if it exists\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters.topics/list\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}/topics\"\n",
        "\n",
        "  # Gather existing clusters\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if cluster exists, if so return\n",
        "  if \"topics\" in json_result:\n",
        "    for item in json_result[\"topics\"]:\n",
        "      print(f\"Apache Kafka for BigQuery Topic: {item['name']}\")\n",
        "      # \"projects/${project_id}/locations/us-central1/clusters/kafka-cluster\"\n",
        "      if item[\"name\"] == f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}/topics/{kafka_topic_name}\":\n",
        "        print(\"Apache Kafka for BigQuery Topic already exists\")\n",
        "        return None\n",
        "\n",
        "\n",
        "  # Create Apache Kafka For BigQuery Topic\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters.topics/create\n",
        "  print(\"Creating Apache Kafka For BigQuery Topic\")\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}/topics?topicId={kafka_topic_name}\"\n",
        "\n",
        "  # partition_count 32 -> for larger cluster\n",
        "  request_body = {\n",
        "      \"partition_count\"    : 6,\n",
        "      \"replication_factor\" : 3\n",
        "    }\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "\n",
        "  name = json_result[\"name\"]\n",
        "  print(\"Apache Kafka for BigQuery Topic created: \", name)\n",
        "  return None"
      ],
      "metadata": {
        "id": "M4AiPPGpG9rl"
      },
      "id": "M4AiPPGpG9rl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deleteApacheKafkaForBigQueryCluster():\n",
        "  \"\"\"Deletes a Apache Kafka For BigQuery Cluster.\"\"\"\n",
        "\n",
        "  # First find the cluster if it exists\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/list\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/clusters\"\n",
        "\n",
        "  # Gather existing clusters\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createApacheKafkaForBigQueryCluster (GET) json_result: {json_result}\")\n",
        "  found = False\n",
        "\n",
        "  # Test to see if cluster, if so then delete\n",
        "  if \"clusters\" in json_result:\n",
        "    for item in json_result[\"clusters\"]:\n",
        "      print(f\"Apache Kafka for BigQuery: {item['name']}\")\n",
        "      # \"projects/${project_id}/locations/us-central1/clusters/kafka-cluster\"\n",
        "      if item[\"name\"] == f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}\":\n",
        "        print(\"Apache Kafka for BigQuery  exists\")\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "  if found == False:\n",
        "    print(\"Apache Kafka for BigQuery does not exist\")\n",
        "    return None\n",
        "\n",
        "  # Create Apache Kafka For BigQuery Cluster\n",
        "  # https://cloud.google.com/managed-kafka/docs/reference/rest/v1/projects.locations.clusters/delete\n",
        "  print(\"Deleting Apache Kafka For BigQuery Cluster\")\n",
        "\n",
        "  url = f\"https://managedkafka.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}\"\n",
        "\n",
        "  json_result = restAPIHelper(url, \"DELETE\", request_body={})\n",
        "\n",
        "  print(\"Apache Kafka for BigQuery deleted\")"
      ],
      "metadata": {
        "id": "QjxFsg11-zUj"
      },
      "id": "QjxFsg11-zUj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kafka Auth"
      ],
      "metadata": {
        "id": "sVtsEzqGYScL"
      },
      "id": "sVtsEzqGYScL"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import datetime\n",
        "import http.server\n",
        "import google.auth\n",
        "import google.auth.crypt\n",
        "import google.auth.jwt\n",
        "import google.auth.transport.urllib3\n",
        "import urllib3\n",
        "from kafka.oauth.abstract import AbstractTokenProvider\n",
        "from kafka.errors import KafkaError\n",
        "\n",
        "class MyTokenProvider(AbstractTokenProvider):\n",
        "\n",
        "    _credentials, _project = google.auth.default(\n",
        "        scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
        "    )\n",
        "    _http_client = urllib3.PoolManager()\n",
        "\n",
        "    def valid_credentials(self):\n",
        "      if not self._credentials.valid:\n",
        "        self._credentials.refresh(google.auth.transport.urllib3.Request(self._http_client))\n",
        "      return self._credentials\n",
        "\n",
        "    _HEADER = json.dumps(dict(typ='JWT', alg='GOOG_OAUTH2_TOKEN'))\n",
        "\n",
        "    def get_jwt(self, creds):\n",
        "      return json.dumps(\n",
        "          dict(\n",
        "              exp=creds.expiry.replace(tzinfo=datetime.timezone.utc).timestamp(),\n",
        "              iss='Google',\n",
        "              iat=datetime.datetime.now(datetime.timezone.utc).timestamp(),\n",
        "              sub=creds.service_account_email,\n",
        "          )\n",
        "      )\n",
        "\n",
        "    def b64_encode(self, source):\n",
        "      return (\n",
        "          base64.urlsafe_b64encode(source.encode('utf-8'))\n",
        "          .decode('utf-8')\n",
        "          .rstrip('=')\n",
        "      )\n",
        "\n",
        "    def get_kafka_access_token(self, creds):\n",
        "      return '.'.join(\n",
        "          [self.b64_encode(self._HEADER), self.b64_encode(self.get_jwt(creds)), self.b64_encode(creds.token)]\n",
        "      )\n",
        "\n",
        "    def build_message(self):\n",
        "      creds = self.valid_credentials()\n",
        "      expiry_seconds = (\n",
        "          creds.expiry.replace(tzinfo=datetime.timezone.utc) - datetime.datetime.now(datetime.timezone.utc)\n",
        "      ).total_seconds()\n",
        "      return self.get_kafka_access_token(creds)\n",
        "\n",
        "    def __init__(self, **config):\n",
        "        pass\n",
        "\n",
        "    def token(self):\n",
        "        message = self.build_message()\n",
        "        return message"
      ],
      "metadata": {
        "id": "0otONJDLdjbi"
      },
      "id": "0otONJDLdjbi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy Data"
      ],
      "metadata": {
        "id": "mzHG3ZSsSak_"
      },
      "id": "mzHG3ZSsSak_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy our data (CSV files).  We want the files in our local bucket with local location.\n",
        "source_path = \"gs://data-analytics-golden-demo/cymbal-consumer-finance/*\"\n",
        "dest_path = f\"gs://{DATA_BUCKET_NAME}/cymbal-consumer-finance/\"\n",
        "print(f\"Copying data from {source_path} to {dest_path}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "!gsutil -m -q cp -r {source_path} {dest_path}\n",
        "print(\"Copy [data] is complete\")"
      ],
      "metadata": {
        "id": "TSSPAoc3SjWK"
      },
      "id": "TSSPAoc3SjWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6fb449e5-5896-4122-9503-00caca58407a",
      "metadata": {
        "id": "6fb449e5-5896-4122-9503-00caca58407a"
      },
      "source": [
        "# Create Table Scripts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"\n",
        "CREATE SCHEMA IF NOT EXISTS cymbal_consumer_finance OPTIONS(location = 'us');\n",
        "\"\"\"\n",
        "\n",
        "RunQuery(sql)"
      ],
      "metadata": {
        "id": "gzqjMNorSsix"
      },
      "id": "gzqjMNorSsix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_streaming_destination_table = \"kafka_events\"\n",
        "sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.{bigquery_streaming_destination_table}`\n",
        "(\n",
        "    customer_id STRING NOT NULL OPTIONS(description=\"customer id\"),\n",
        "    transaction_or_search STRING NOT NULL OPTIONS(description=\"a user transaction or a search term\")\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "RunQuery(sql)"
      ],
      "metadata": {
        "id": "3UB4WswFAanE"
      },
      "id": "3UB4WswFAanE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_predicted_event_table = \"predicted_life_event\"\n",
        "sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.{bigquery_predicted_event_table}`\n",
        "(\n",
        "    customer_id STRING NOT NULL OPTIONS(description=\"customer id\"),\n",
        "    predicted_life_event STRING NOT NULL OPTIONS(description=\"ML predicted life event based on search terms\")\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "RunQuery(sql)"
      ],
      "metadata": {
        "id": "UAQFJumYOln8"
      },
      "id": "UAQFJumYOln8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "542dbc46-27be-4d36-b60f-0c371419555c"
      },
      "outputs": [],
      "source": [
        "bigquery_ml_training_table = \"transactions_to_life_events\"\n",
        "sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.{bigquery_ml_training_table}`\n",
        "(\n",
        "    transaction_or_search   STRING NOT NULL OPTIONS(description=\"a user transaction or a search term\"),\n",
        "    life_event STRING NOT NULL OPTIONS(description=\"possible life event related to the transaction or search term\"),\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "RunQuery(sql)"
      ],
      "id": "542dbc46-27be-4d36-b60f-0c371419555c"
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_customers_table = \"customers\"\n",
        "sql = f\"\"\"LOAD DATA OVERWRITE `{PROJECT_ID}.{DATASET_ID}.{bigquery_customers_table}`\n",
        "(\n",
        "  customer_id STRING,\n",
        "  first_name STRING,\n",
        "  last_name STRING,\n",
        "  date_of_birth DATE,\n",
        "  email STRING,\n",
        "  phone_number STRING,\n",
        "  creation_date DATE,\n",
        "  life_event STRING\n",
        ")\n",
        "FROM FILES (format = 'CSV', skip_leading_rows = 1, uris = ['gs://{DATA_BUCKET_NAME}/cymbal-consumer-finance/ccf_csv_tables_customers.csv']);\n",
        "\"\"\"\n",
        "RunQuery(sql)"
      ],
      "metadata": {
        "id": "q4MA8G5zRMNq"
      },
      "id": "q4MA8G5zRMNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c90bbf40-115a-4096-9bfd-3af09d13d7d7"
      },
      "source": [
        "# Run Gemini to generate Syntectic Data"
      ],
      "id": "c90bbf40-115a-4096-9bfd-3af09d13d7d7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate an API key, please follow the steps [here](https://support.google.com/googleapi/answer/6158862?hl=en), then enter the key in the box below.\n",
        "\n"
      ],
      "metadata": {
        "id": "XWUMZMGsdWz-"
      },
      "id": "XWUMZMGsdWz-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0fe70feb-7fe5-4ffb-a2d2-f1dc492762c4"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = \"<API_KEY>\""
      ],
      "id": "0fe70feb-7fe5-4ffb-a2d2-f1dc492762c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "adaa81d7-d344-4cb8-912a-75e3d608d0a9"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
        "\n",
        "model = genai.GenerativeModel(model_name='gemini-1.5-pro', generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1\n",
        "    ))"
      ],
      "id": "adaa81d7-d344-4cb8-912a-75e3d608d0a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ee8b30d4-d0e8-4046-802b-048852e8d1b4"
      },
      "outputs": [],
      "source": [
        "count = 100\n",
        "\n",
        "schema = GetTableSchema(DATASET_ID, bigquery_ml_training_table)"
      ],
      "id": "ee8b30d4-d0e8-4046-802b-048852e8d1b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4a57da3e-af70-48d2-b25b-4f036e74d892"
      },
      "outputs": [],
      "source": [
        "#TODO: change this hardcoding.\n",
        "life_events = [\"New Child\",\n",
        "               \"Graduation\",\n",
        "               \"Relocation\",\n",
        "               \"Retirement\",\n",
        "               \"Home Purchase\",\n",
        "               \"Medical Event\",\n",
        "               \"Starting Business\",\n",
        "               \"Marriage\",\n",
        "               \"Divorce\" ]"
      ],
      "id": "4a57da3e-af70-48d2-b25b-4f036e74d892"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "aed06ff4-6b14-4ad9-b008-e606db8493c2"
      },
      "outputs": [],
      "source": [
        "prompt=f\"\"\"\n",
        "You are a database engineer and need to generate data for a table for the below schema.\n",
        "- The schema is for a Google Cloud BigQuery Table.\n",
        "- The table name is \"{PROJECT_ID}.{DATASET_ID}.{bigquery_ml_training_table}\".\n",
        "- Read the description of each field for valid values.\n",
        "- Do not preface the response with any special characters or 'sql'.\n",
        "- Generate {count} insert statements for this table.\n",
        "- Valid values for life events are: {life_events}\n",
        "- Generate values for insert statement such that each life_event has equal number of rows.\n",
        "- For each life event create unique transaction or search term related to that event.\n",
        "\n",
        "Example 1: INSERT INTO `my-dataset.my-dataset.my-table` (field_1, field_2) VALUES (1, 'Sample'),(2, 'Sample');\n",
        "Example 2: INSERT INTO `my-dataset.my-dataset.my-table` (field_1, field_2) VALUES (1, 'Data'),(2, 'Data'),(3, 'Data');\n",
        "\n",
        "Schema: {schema}\n",
        "\"\"\"\n",
        "\n",
        "llm_valid_execution = False\n",
        "loop_count = 0\n",
        "while loop_count < 7:\n",
        "  try:\n",
        "    #sql = LLM(prompt, False, max_output_tokens=1024, temperature=1, top_p=1, top_k=40)\n",
        "    response_json = model.generate_content(prompt)\n",
        "    sql = response_json.text\n",
        "\n",
        "    print(\"---------------------------------\")\n",
        "    print(\"sql: \", sql)\n",
        "    print(\"---------------------------------\")\n",
        "    loop_count += 1\n",
        "    llm_valid_execution = RunQuery(sql)\n",
        "    if not llm_valid_execution:\n",
        "        print(\"Error running generated SQL\")\n",
        "        break;\n",
        "  except Exception as error:\n",
        "    print(\"An error occurred:\", error)"
      ],
      "id": "aed06ff4-6b14-4ad9-b008-e606db8493c2"
    },
    {
      "cell_type": "markdown",
      "id": "a30b78d6-2438-4c1c-b027-25abae64f6d1",
      "metadata": {
        "id": "a30b78d6-2438-4c1c-b027-25abae64f6d1"
      },
      "source": [
        "# Create a BQ Gemini Model to help with prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the required permissions on the external connection's service principal\n",
        "vertexAIServiceAccountId = createVertexAIConnection(\"vertex-ai\")\n",
        "\n",
        "vertexAIServiceAccountId\n",
        "\n",
        "# To call GENERATE TEXT\n",
        "setProjectLevelIamPolicy(f\"serviceAccount:{vertexAIServiceAccountId}\",\"roles/aiplatform.user\")"
      ],
      "metadata": {
        "id": "SX2aiRq3eqvV"
      },
      "id": "SX2aiRq3eqvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{DATASET_ID}.gemini_model`\n",
        "REMOTE WITH CONNECTION `projects/{PROJECT_ID}/locations/{BQ_LOCATION}/connections/{CONNECTION_NAME}`\n",
        "OPTIONS (ENDPOINT = 'gemini-1.5-flash-002');\n",
        "\"\"\"\n",
        "RunQuery(sql)"
      ],
      "metadata": {
        "id": "T4BfJ4p1heGa"
      },
      "id": "T4BfJ4p1heGa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create synthetic data for Kafka"
      ],
      "metadata": {
        "id": "_6SgDFXD0Qwe"
      },
      "id": "_6SgDFXD0Qwe"
    },
    {
      "cell_type": "code",
      "source": [
        "#change the project name to {PROJECT_ID}\n",
        "\n",
        "sql = f\"\"\"SELECT DISTINCT customer_id FROM `{PROJECT_ID}.{DATASET_ID}.{bigquery_customers_table}` LIMIT 3000\"\"\"\n",
        "customer_ids_df = RunQuery(sql)\n",
        "\n",
        "#get a few transaction/searches we created before\n",
        "\n",
        "sql = f\"\"\"SELECT DISTINCT transaction_or_search FROM `{PROJECT_ID}.{DATASET_ID}.{bigquery_ml_training_table}` LIMIT 1000\"\"\"\n",
        "searches_df = RunQuery(sql)"
      ],
      "metadata": {
        "id": "F1V-NPhL0ZEa"
      },
      "id": "F1V-NPhL0ZEa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Convert DataFrames to lists for easier random selection\n",
        "customer_ids = customer_ids_df['customer_id'].tolist()  # Access the column by name\n",
        "searches = searches_df['transaction_or_search'].tolist() # Access the column by name\n",
        "\n",
        "num_records = 10\n",
        "\n",
        "combined_records = []\n",
        "\n",
        "for _ in range(num_records):\n",
        "    customer_id = random.choice(customer_ids)\n",
        "    search = random.choice(searches)\n",
        "    record = {\n",
        "        \"customer_id\": customer_id,\n",
        "        \"transaction_or_search\": search\n",
        "        }\n",
        "    combined_records.append(record)"
      ],
      "metadata": {
        "id": "uPj80E-K5_tA"
      },
      "id": "uPj80E-K5_tA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3ad542e8-28f9-4bc5-80b4-b049f4aa647e",
      "metadata": {
        "id": "3ad542e8-28f9-4bc5-80b4-b049f4aa647e"
      },
      "source": [
        "# Create Kafka Infra"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To see your clusters: https://console.cloud.google.com/managedkafka/clusterList\n",
        "\n",
        "opertion = createApacheKafkaForBigQueryCluster()\n",
        "\n",
        "if opertion is not None:\n",
        "  waitForApacheKafkaForBigQueryCluster(opertion)"
      ],
      "metadata": {
        "id": "f_H2q7h20Dv0"
      },
      "id": "f_H2q7h20Dv0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create kafka topic\n",
        "\n",
        "createApacheKafkaForBigQueryTopic()"
      ],
      "metadata": {
        "id": "riNpOh3pHtW5"
      },
      "id": "riNpOh3pHtW5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send data to Kafka"
      ],
      "metadata": {
        "id": "AodqGtGyCBEb"
      },
      "id": "AodqGtGyCBEb"
    },
    {
      "cell_type": "code",
      "source": [
        "# send data to the newly created kafka topic\n",
        "from kafka import KafkaProducer\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "\n",
        "mtp = MyTokenProvider()\n",
        "bootstrap_server = f\"bootstrap.{kafka_cluster_name}.{REGION}.managedkafka.{PROJECT_ID}.cloud.goog:9092\"\n",
        "\n",
        "# Create a Kafka producer\n",
        "producer = KafkaProducer(\n",
        "        client_id=\"one-file-writer\",\n",
        "        bootstrap_servers=bootstrap_server,\n",
        "        security_protocol='SASL_SSL',\n",
        "        sasl_mechanism='OAUTHBEARER',\n",
        "        sasl_oauth_token_provider = mtp,\n",
        "        api_version=(1,4,7),\n",
        "        api_version_auto_timeout_ms\t= 20000,\n",
        "        request_timeout_ms = 60000,\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8') )"
      ],
      "metadata": {
        "id": "BbPhmmBqH7pu"
      },
      "id": "BbPhmmBqH7pu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through combined_records and send to Kafka\n",
        "\n",
        "for record in combined_records:\n",
        "  #json_data = json.dumps(record)\n",
        "  # Send the JSON data to Kafka\n",
        "  producer.send(kafka_topic_name, record)\n",
        "\n",
        "producer.flush()\n",
        "producer.close()\n",
        "print(f\"Sent all messages to Kafka\")"
      ],
      "metadata": {
        "id": "2ADQazg3wA5b"
      },
      "id": "2ADQazg3wA5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test reading Kafka data"
      ],
      "metadata": {
        "id": "f4WS6rJvILy6"
      },
      "id": "f4WS6rJvILy6"
    },
    {
      "cell_type": "code",
      "source": [
        "#test reading kafka messages\n",
        "\n",
        "# Create a Kafka consumer\n",
        "consumer = KafkaConsumer( kafka_topic_name,  # Replace with the topic you're consuming from\n",
        "    bootstrap_servers=bootstrap_server,\n",
        "    security_protocol='SASL_SSL',\n",
        "    sasl_mechanism='OAUTHBEARER',\n",
        "    sasl_oauth_token_provider=mtp,\n",
        "    api_version=(1, 4, 7),  # Must match the producer's API version or be compatible\n",
        "    api_version_auto_timeout_ms=20000,\n",
        "    request_timeout_ms=60000,\n",
        "    value_deserializer=lambda v: json.loads(v.decode('utf-8')),  # Important: Deserialize JSON\n",
        "    auto_offset_reset='earliest', # Or 'latest' depending on your needs. Important!\n",
        "    enable_auto_commit=True, # or False, depending on your needs. Important!\n",
        "    consumer_timeout_ms=10000 # How long to wait for messages before timing out.\n",
        ")\n",
        "\n",
        "try:\n",
        "    for message in consumer:\n",
        "        # Process the received message\n",
        "        print(f\"Received message: {message.value}\")\n",
        "        # ... your message processing logic here ...\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Consumer stopped by user.\")\n",
        "\n",
        "finally:\n",
        "    consumer.close()\n",
        "    print(\"Consumer closed.\")\n"
      ],
      "metadata": {
        "id": "f9mrWssMtBsg"
      },
      "id": "f9mrWssMtBsg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stream kafka events to BQ via Dataflow"
      ],
      "metadata": {
        "id": "iMO0UpJ-BDNT"
      },
      "id": "iMO0UpJ-BDNT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNING: This will create a new job everytime this is run. The notebook will only stop the lastest job, so please check the DataFlow UI to Cancel any additional jobs.\n"
      ],
      "metadata": {
        "id": "lOVahddaBmwm"
      },
      "id": "lOVahddaBmwm"
    },
    {
      "cell_type": "code",
      "source": [
        "def createDataflowJobApacheKafkaToBigQuery(jobName):\n",
        "  \"\"\"Creates a DataFlow job to copy data from Apache Kafka for BiqQuery to stream data into a BigQuery Table\"\"\"\n",
        "\n",
        "  # First find the job if it exists\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/list\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{PROJECT_ID}/jobs?location={REGION}\"\n",
        "\n",
        "  # Gather existing job\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"createDataflowJobApacheKafkaToBigQuery (GET) json_result: {json_result}\")\n",
        "\n",
        "  # Test to see if job exists, if so return\n",
        "  if \"jobs\" in json_result:\n",
        "    for item in json_result[\"jobs\"]:\n",
        "      print(f\"DataFlow Job Name: {item['name']}\")\n",
        "      if item[\"name\"] == jobName:\n",
        "        print(f\"DataFlow job already exists with date of {item['currentState']}.  Try a new name.\")\n",
        "        return None\n",
        "\n",
        "  # Create DataFlow Job\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch\n",
        "  print(\"Creating DataFlow Job from Flex Template\")\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{PROJECT_ID}/locations/{REGION}/flexTemplates:launch\"\n",
        "\n",
        "  # Continuous Queries needs useStorageWriteApiAtLeastOnce = True\n",
        "  # https://cloud.google.com/dataflow/docs/guides/templates/provided/kafka-to-bigquery#optional-parameters\n",
        "  #numStorageWriteApiStreams : Specifies the number of write streams, this parameter must be set. Default is 0.\n",
        "  #storageWriteApiTriggeringFrequencySec : Specifies the triggering frequency in seconds, this parameter must be set. Default is 5 seconds.\n",
        "  #useStorageWriteApiAtLeastOnce : This parameter takes effect only if \"Use BigQuery Storage Write API\" is enabled. If enabled the at-least-once semantics will be used for Storage Write API, otherwise exactly-once semantics will be used. Defaults to: false.\n",
        "\n",
        "  request_body = {\n",
        "    \"launch_parameter\": {\n",
        "        \"jobName\": jobName,\n",
        "        \"containerSpecGcsPath\": \"gs://dataflow-templates-us-central1/latest/flex/Kafka_to_BigQuery_Flex\",\n",
        "        \"parameters\": {\n",
        "            \"readBootstrapServerAndTopic\": f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{kafka_cluster_name}/topics/{kafka_topic_name}\",\n",
        "            \"persistKafkaKey\": \"false\",\n",
        "            \"writeMode\": \"SINGLE_TABLE_NAME\",\n",
        "            \"numStorageWriteApiStreams\": \"2\",\n",
        "            \"useStorageWriteApiAtLeastOnce\": \"true\",\n",
        "            \"storageWriteApiTriggeringFrequencySec\": \"5\",\n",
        "            \"enableCommitOffsets\": \"false\",\n",
        "            \"kafkaReadOffset\": \"latest\",\n",
        "            \"kafkaReadAuthenticationMode\": \"APPLICATION_DEFAULT_CREDENTIALS\",\n",
        "            \"messageFormat\": \"JSON\",\n",
        "            \"useBigQueryDLQ\": \"false\",\n",
        "            \"stagingLocation\": f\"gs://{dataflow_bucket}/staging\",\n",
        "            \"autoscalingAlgorithm\": \"NONE\",\n",
        "            \"serviceAccount\": dataflow_service_account,\n",
        "            \"usePublicIps\": \"false\",\n",
        "            \"labels\": \"{\\\"goog-dataflow-provided-template-type\\\":\\\"flex\\\",\\\"goog-dataflow-provided-template-name\\\":\\\"kafka_to_bigquery_flex\\\"}\",\n",
        "            \"outputTableSpec\": f\"{PROJECT_ID}:{DATASET_ID}.{bigquery_streaming_destination_table}\"\n",
        "        },\n",
        "        \"environment\": {\n",
        "            \"numWorkers\": 2,\n",
        "            \"tempLocation\": f\"gs://{dataflow_bucket}/tmp\",\n",
        "            \"subnetwork\": f\"regions/{REGION}/subnetworks/{subnet}\",\n",
        "            \"enableStreamingEngine\": True,\n",
        "            \"additionalExperiments\": [\n",
        "                \"enable_streaming_engine\"\n",
        "            ],\n",
        "            \"additionalUserLabels\": {}\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "  json_result = restAPIHelper(url, \"POST\", request_body)\n",
        "\n",
        "  job = json_result[\"job\"]\n",
        "  print(\"Dataflow job for kakfa->BQ created: \", job)\n",
        "  return job"
      ],
      "metadata": {
        "id": "b3lUeWItL9m5"
      },
      "id": "b3lUeWItL9m5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopDataflowJobApacheKafkaToBigQuery(jobName):\n",
        "  \"\"\"Stops a DataFlow job to copy data from Apache Kafka for BiqQuery to stream data into a BigQuery Table\"\"\"\n",
        "\n",
        "  # First find the job if it exists\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/list\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{PROJECT_ID}/jobs?location={REGION}\"\n",
        "\n",
        "  # Gather existing jobs\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"stopDataflowJobApacheKafkaToBigQuery (GET) json_result: {json_result}\")\n",
        "  found = False\n",
        "\n",
        "  # Test to see if job exists, if so return\n",
        "  if \"jobs\" in json_result:\n",
        "    for item in json_result[\"jobs\"]:\n",
        "      print(f\"DataFlow Job Name: {item['name']} - {item['currentState']}\")\n",
        "      if item[\"name\"] == jobName and item[\"currentState\"] == \"JOB_STATE_RUNNING\":\n",
        "        jobId = item[\"id\"]\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "  if not found:\n",
        "    print(\"DataFlow not found or is not running.\")\n",
        "    return\n",
        "\n",
        "  # Stop DataFlow Job\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/update\n",
        "  print(\"Stopping DataFlow Job \")\n",
        "\n",
        "  url=f\"https://dataflow.googleapis.com/v1b3/projects/{PROJECT_ID}/locations/{REGION}/jobs/{jobId}\"\n",
        "  print(url)\n",
        "\n",
        "  request_body = { \"requestedState\" : \"JOB_STATE_CANCELLED\" }\n",
        "\n",
        "  json_result = restAPIHelper(url, \"PUT\", request_body)\n",
        "\n",
        "  #job = json_result[\"job\"]\n",
        "  print(\"DataFlow Job Stopped\")\n",
        "  return"
      ],
      "metadata": {
        "id": "7Jide8MAMjyD"
      },
      "id": "7Jide8MAMjyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def waitForDataFlowJobToStart(jobName):\n",
        "  \"\"\"Waits for job to turn to running\"\"\"\n",
        "\n",
        "  # First find the job if it exists\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/list\n",
        "\n",
        "  url = f\"https://dataflow.googleapis.com/v1b3/projects/{PROJECT_ID}/jobs?location={REGION}\"\n",
        "\n",
        "  # Gather existing jobs\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  print(f\"stopDataflowJobApacheKafkaToBigQuery (GET) json_result: {json_result}\")\n",
        "  found = False\n",
        "\n",
        "  # Test to see if job exists, if so return\n",
        "  if \"jobs\" in json_result:\n",
        "    for item in json_result[\"jobs\"]:\n",
        "      print(f\"DataFlow Job Name: {item['name']} - {item['currentState']}\")\n",
        "      if item[\"name\"] == jobName:\n",
        "        jobId = item[\"id\"]\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "  if not found:\n",
        "    print(\"DataFlow not found or is not running.\")\n",
        "    return\n",
        "\n",
        "  # Gets the job status\n",
        "  # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/get\n",
        "  print(\"Getting DataFlow Job \")\n",
        "  url=f\"https://dataflow.googleapis.com/v1b3/projects/{PROJECT_ID}/locations/{REGION}/jobs/{jobId}\"\n",
        "  print(url)\n",
        "\n",
        "  max_retries = 100\n",
        "  attempt = 0\n",
        "\n",
        "  while True:\n",
        "    # Get Job\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "\n",
        "    # Test to see if connection exists, if so return\n",
        "    if \"currentState\" in json_result:\n",
        "      print(f\"waitForDataFlowJobToStart (GET) currentState: {json_result['currentState']}\")\n",
        "      if json_result[\"currentState\"] == \"JOB_STATE_RUNNING\":\n",
        "        print(\"DataFlow Job is now running\")\n",
        "        return None\n",
        "    else:\n",
        "      print(f\"waitForDataFlowJobToStart (GET) json_result: {json_result}\")\n",
        "\n",
        "\n",
        "    # Wait for 10 seconds\n",
        "    attempt += 1\n",
        "    if attempt > max_retries:\n",
        "      raise RuntimeError(\"DataFlow Job not created\")\n",
        "    time.sleep(30)"
      ],
      "metadata": {
        "id": "_9KGk2IoMntk"
      },
      "id": "_9KGk2IoMntk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#give the dataflow service account data owner access on the dataset in BQ\n",
        "\n",
        "setBigQueryDatasetPolicy(f\"{dataflow_service_account}\", \"OWNER\")"
      ],
      "metadata": {
        "id": "_wfE8csY6sER"
      },
      "id": "_wfE8csY6sER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The job can take a few minutes to start.  Click the link to see the progress:\n",
        "# https://console.cloud.google.com/dataflow/jobs\n",
        "\n",
        "jobName= f\"kafka-stream-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
        "createDataflowJobApacheKafkaToBigQuery(jobName)"
      ],
      "metadata": {
        "id": "FIOHaQGrMtxq"
      },
      "id": "FIOHaQGrMtxq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waitForDataFlowJobToStart(jobName)"
      ],
      "metadata": {
        "id": "WE3WyujgMugU"
      },
      "id": "WE3WyujgMugU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use continuous queries to detect life event with BQ ML"
      ],
      "metadata": {
        "id": "sxplX1aJBJzP"
      },
      "id": "sxplX1aJBJzP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a continuous query reservation and assignment\n",
        "\n",
        "user_input = input(\"Do you want to start BigQuery Reservations? This will START billing and will continue until you remove these in the Clean Up code (Y/n)?\")\n",
        "if user_input == \"Y\":\n",
        "  sql = f\"\"\"CREATE RESERVATION `{PROJECT_ID}.region-US.continuous-query-reservation`\n",
        "            OPTIONS (edition = \"enterprise\",\n",
        "                     slot_capacity = 50);\n",
        "  \"\"\"\n",
        "  RunQuery(sql)\n",
        "\n",
        "  sql = f\"\"\"CREATE ASSIGNMENT `{PROJECT_ID}.region-US.continuous-query-reservation.continuous-query-reservation-assignment`\n",
        "            OPTIONS(assignee = \"projects/{PROJECT_ID}\",\n",
        "                    job_type = \"CONTINUOUS\");\n",
        "  \"\"\"\n",
        "  RunQuery(sql)"
      ],
      "metadata": {
        "id": "esZ8S6k88D1q"
      },
      "id": "esZ8S6k88D1q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run each of the below queries**\n",
        "\n",
        "* Copy the SQL to a BigQuery SQL Window\n",
        "* Under the More menu, select Continuous Query\n",
        "* Under the Query settings, under Continuous query, select kafka-continuous-query for the service account\n",
        "* Run the Query (it will take a minute to start)"
      ],
      "metadata": {
        "id": "oeXf7X_Q8J6h"
      },
      "id": "oeXf7X_Q8J6h"
    },
    {
      "cell_type": "code",
      "source": [
        "# continuous query for predicting life_event on the stremed data\n",
        "\n",
        "sql = f\"\"\"\n",
        "INSERT INTO `{PROJECT_ID}.{DATASET_ID}.predicted_life_event` (\n",
        "\n",
        "\n",
        "with streaming_data AS (\n",
        "SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.kafka_events`\n",
        "),\n",
        "prediction AS (\n",
        "  SELECT *\n",
        "    FROM ML.GENERATE_TEXT(MODEL`{PROJECT_ID}.{DATASET_ID}.gemini_model`,\n",
        "          (SELECT customer_id,transaction_or_search,\n",
        "                             CONCAT(\"Generate a life event from the give set of keywords that a customer used for searching. for example, a keyword such \",\n",
        "                                    \"'best sofa for living room' may suggest the customer bought a new house. so the life event is 'New House' \",\n",
        "                                    \"The kewords for which you need to predict the life event is: \", transaction_or_search,\n",
        "                                    \"Only possible life events are: New Child,Graduation,Relocation,\",\n",
        "                                    \"Retirement, Home Purchase,Medical Event,Starting Business,Marriage,Divorce.\",\n",
        "                                    \"ouput the life event as the only output, strip all other result sentences\") AS prompt\n",
        "            FROM streaming_data),\n",
        "          STRUCT(.8 AS temperature, .8 AS top_p)\n",
        "          )\n",
        ")\n",
        "\n",
        "select customer_id,\n",
        "STRING(ml_generate_text_result.candidates[0].content.parts[0].text) as predicted_life_event\n",
        "from prediction\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YXudLqkqG33u"
      },
      "id": "YXudLqkqG33u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Up"
      ],
      "metadata": {
        "id": "FzMeIYkV-UnG"
      },
      "id": "FzMeIYkV-UnG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note if you do not know your job id, or overwrote the value, click here to open and manually Cancel the job\n",
        "# https://console.cloud.google.com/dataflow/jobs\n",
        "\n",
        "user_input = input(f\"Do you want to delete your DataFlow Job {jobName} (Y/n)?\")\n",
        "if user_input == \"Y\":\n",
        "  stopDataflowJobApacheKafkaToBigQuery(jobName)"
      ],
      "metadata": {
        "id": "ysn2NdDd-ZFR"
      },
      "id": "ysn2NdDd-ZFR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Do you want to delete your Apache Kafka for BigQuery (Y/n)?\")\n",
        "if user_input == \"Y\":\n",
        "  deleteApacheKafkaForBigQueryCluster()"
      ],
      "metadata": {
        "id": "f-78zcDJ-deg"
      },
      "id": "f-78zcDJ-deg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user_input = input(\"Do you want to delete your BigQuery Reservations. This will STOP billing! (Y/n)?\")\n",
        "if user_input == \"Y\":\n",
        "  sql = f\"DROP ASSIGNMENT `{PROJECT_ID}.region-{REGION}.continuous-query-reservation.continuous-query-reservation-assignment`;\"\n",
        "  RunQuery(sql)\n",
        "  sql = f\"DROP RESERVATION `{PROJECT_ID}.region-{REGION}.continuous-query-reservation`;\"\n",
        "  RunQuery(sql)"
      ],
      "metadata": {
        "id": "-xE0E_TI-fnm"
      },
      "id": "-xE0E_TI-fnm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-cpu.2-11.m125",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
    },
    "kernelspec": {
      "display_name": "Python 3 (Local)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": true,
    "colab": {
      "provenance": [],
      "name": "Tech Immersion - Streaming",
      "collapsed_sections": [
        "NFGz8rDcnk7v",
        "_dv8JuvQnd-k",
        "65fff23c-4782-4324-93e6-0684d8476d11",
        "mzHG3ZSsSak_",
        "6fb449e5-5896-4122-9503-00caca58407a",
        "c90bbf40-115a-4096-9bfd-3af09d13d7d7",
        "a30b78d6-2438-4c1c-b027-25abae64f6d1",
        "_6SgDFXD0Qwe",
        "3ad542e8-28f9-4bc5-80b4-b049f4aa647e",
        "f4WS6rJvILy6",
        "iMO0UpJ-BDNT",
        "sxplX1aJBJzP",
        "FzMeIYkV-UnG"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}